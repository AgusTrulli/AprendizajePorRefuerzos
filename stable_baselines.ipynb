{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créditos:\n",
    "\n",
    "* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.\n",
    "    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.\n",
    "* Documentación y repo de OpenAI Gym https://github.com/openai/gym/blob/master/docs/.\n",
    "    * Crear un entorno https://github.com/openai/gym/blob/master/docs/creating-environments.md. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL \"al estilo scikit-learn\". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$\n",
    "\n",
    "Además, ofrece herramientas muy útiles como\n",
    "\n",
    "* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.\n",
    "* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).\n",
    "\n",
    "\n",
    "Documentación https://stable-baselines3.readthedocs.io\n",
    "\n",
    "Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)\n",
    "\n",
    "La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional\n",
    "\n",
    "$^1$ no obstante, al igual que sucede generalmente con librerías de ML: \n",
    "\n",
    "* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. \n",
    "\n",
    "* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfaz básica stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación\n",
    "\n",
    "Desde Linux o Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (0.11.0a2)\n",
      "Requirement already satisfied: numpy in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (1.18.4)\n",
      "Requirement already satisfied: gym>=0.17 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (0.17.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (1.5.0)\n",
      "Requirement already satisfied: pandas in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (1.0.3)\n",
      "Requirement already satisfied: cloudpickle in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (1.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (3.2.1)\n",
      "Requirement already satisfied: psutil; extra == \"extra\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (5.7.3)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (0.2.6)\n",
      "Requirement already satisfied: tensorboard; extra == \"extra\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (2.4.0)\n",
      "Requirement already satisfied: pillow; extra == \"extra\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (7.1.2)\n",
      "Requirement already satisfied: opencv-python; extra == \"extra\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
      "Requirement already satisfied: scipy in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
      "Requirement already satisfied: future in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.17.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from pandas->stable-baselines3[extra]) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from pandas->stable-baselines3[extra]) (2019.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3[extra]) (2.4.0)\n",
      "Requirement already satisfied: six in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.14.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.21.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.33.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (46.4.0.post20200518)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2020.4.5.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.7.4.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de un algoritmo de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones/inicializaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "#from gym.envs.registration import register\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f7cc01adbd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# MlpPolicy es una política \"estándar\" que aprende con perceptron multicapa\n",
    "# (es decir sin capas convolucionales o demás variantes),\n",
    "# 2 capas ocultas con 64 neuronas cada una\n",
    "model = DQN('MlpPolicy', env)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ver rendimiento del agente en tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f7c7d57a050>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = DQN('MlpPolicy', env, tensorboard_log='tensorboard/')\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verlo en tensorboard, correr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorboard --logdir=tensorboard/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** corroborar en windoors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor\n",
    "\n",
    "Vamos a crear un monitor para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f7c745503d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, 'logs/')  # reemplazamos env por su monitor\n",
    "\n",
    "model = DQN('MlpPolicy', env, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=9.60 +/- 0.80\n",
      "Episode length: 9.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "Eval num_timesteps=3000, episode_reward=9.80 +/- 0.75\n",
      "Episode length: 9.80 +/- 0.75\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/python37/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f7c7973d690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "callbacks = []  # lista de callbacks a usar, pueden ser varios\n",
    "\n",
    "# callback para detener entrenamiento al alcanzar recompensa de 9.8\n",
    "# (a fines demostrativos, es una recompensa baja)\n",
    "stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)\n",
    "\n",
    "# al crear EvalCallback, se asocia el mismo con stop_training_callback\n",
    "callbacks.append(EvalCallback(env, \n",
    "                              eval_freq=1000,\n",
    "                              callback_on_new_best=stop_training_callback))\n",
    "\n",
    "# la semilla aleatoria hace que las ejecuciones sean estocásticas\n",
    "model = DQN('MlpPolicy', env, seed=42)\n",
    "model.learn(total_timesteps=10000, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar agente RL en múltiples ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.\n",
    "\n",
    "Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.\n",
    "\n",
    "Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SB brinda dos formas de utilizar entornos vectorizados:\n",
    "\n",
    "* DummyVecEnv, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados (ejemplo: el *stacking* de 4 imágenes en entornos de tipo Atari).\n",
    "* SubprocVecEnv, el cuál agrupa varios entornos que serán ejecutados en paralelo. Atención! **Puede comer mucha RAM**\n",
    "\n",
    "Vemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7c7414b8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de ambiente dummy\n",
    "venv = DummyVecEnv([lambda: gym.make('CartPole-v1')]*4)\n",
    "\n",
    "model = PPO('MlpPolicy', venv, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puede hacerse con un una función de SB a tal efecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7c7c1a5c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv = make_vec_env(lambda: env, n_envs=4)\n",
    "\n",
    "model = PPO('MlpPolicy', venv, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar agente con políticas personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7c745568d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una clase con una red neuronal de 128x128 neuronas\n",
    "\n",
    "model = PPO('MlpPolicy', policy_kwargs=dict(net_arch=[128,128]), env='CartPole-v1', verbose=1).learn(total_timesteps=10000)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar un entorno personalizado\n",
    "\n",
    "Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:\n",
    "\n",
    "* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la \"forma\" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.\n",
    "* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.\n",
    "* Los espacios discretos definen un conjunto de $n$ estados/acciones $\\{ 0, 1, \\dots, n-1 \\}$, mientras que los espacios continuos definen un espacio $\\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\\infty, b], [a, \\infty), (-\\infty, \\infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).\n",
    "* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\\{0,1,2,3\\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como\n",
    "\n",
    "`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO translate below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left. \n",
    "  \"\"\"\n",
    "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Important: the observation must be a numpy array\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Account for the boundaries of the grid\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Are we at the left of the grid?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1).learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: extender este entorno. Algunas ideas:\n",
    "\n",
    "* Transformarlo en una grilla 2D, añadir paredes/trampas/agua.\n",
    "* **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-baselines zoo\n",
    "\n",
    "Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.\n",
    "\n",
    "Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "    !cd rl-baselines3-zoo/\n",
    "    !apt-get install swig cmake ffmpeg\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Linux, ejecutando\n",
    "\n",
    "    git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "    cd rl-baselines3-zoo/\n",
    "    sudo apt-get install swig cmake ffmpeg\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución\n",
    "\n",
    "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
    "\n",
    "`python train.py --algo algo_name --env env_id`\n",
    "\n",
    "Los cuales pueden ser llamados usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('rl-baselines3-zoo/')\n",
    "\n",
    "args = [\n",
    "    '-n', str(100000),\n",
    "    '--algo', 'ppo',\n",
    "    '--env', 'CartPole-v1'\n",
    "]\n",
    "\n",
    "return_code = subprocess.call(['python', 'train.py'] + args)\n",
    "os.chdir(cwd)\n",
    "assert return_code == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver en acción el agente entrenado (nota: no disponible en Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    pass  # TODO mostrar el uso del enjoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabar video! **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver curva de aprendizaje obtenida por el agente desde *utils.plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juan/Documents/RLDiplodatos\n"
     ]
    }
   ],
   "source": [
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('rl-baselines3-zoo/')\n",
    "\n",
    "args = [\n",
    "    'ppo',\n",
    "    'CartPole-v1',\n",
    "    'logs/',\n",
    "    'steps'\n",
    "]\n",
    "\n",
    "return_code = subprocess.call(['python', '-m', 'scripts.plot_train'] + args, stdout=subprocess.PIPE)\n",
    "\n",
    "os.chdir(cwd)\n",
    "assert return_code == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de features y recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Híper-parámetros\n",
    "\n",
    "RL-Baselines Zoo provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).\n",
    "\n",
    "Nota: **Consume muchos recursos!**\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursos adicionales\n",
    "\n",
    "* Framework adicional de aprendizaje por refuerzos a gran escala: https://docs.ray.io/en/master/rllib.html.\n",
    "* Awesome Deep RL: https://github.com/kengz/awesome-deep-rl\n",
    "\n",
    "* **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
